{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dense_unet_3d import DUnet\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import scipy.io as io\n",
    "import statistics\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "size1 = 128\n",
    "size2 = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "#Decoding layer: Same as traditional 3D Unet--upsamples total number of features except in slice directions\n",
    "def decoder_layer(input_, x, ch, name):\n",
    "        \n",
    "\n",
    "    up = tf.layers.conv3d_transpose(input_,filters=12,kernel_size = [2,2,1],strides = [2,2,1],padding='SAME',name='upsample'+str(name), use_bias=False)\n",
    "    up = tf.concat([up,x], axis=-1, name='merge'+str(name))\n",
    "    return up\n",
    "#################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#Encoding layer: Differences from Dense-Unet -- uses growth of 12 (ie number of channels at each convolution layer) \n",
    "#and iterates a number times and concatenates each feature map\n",
    "\n",
    "#Currently pool size and kernal size are each set to [2x2x1] so that all features maps are halved at the height and width \n",
    "#but no the slice number--this allows for variable input sizes\n",
    "\n",
    "# example: x_con.shape = [1,H,W,S]; after max-pooling with pool_size = [2x2x1], kernal = [2x2x1] --> x_con.shape = [1, H/2, W/2, S] \n",
    "def encoder_layer(x_con, iterations, name,training, pool=True):\n",
    "    \n",
    "    with tf.name_scope(\"encoder_block_{}\".format(name)):\n",
    "        for i in range(iterations):\n",
    "            \n",
    "            x = tf.layers.conv3d(x_con,12,kernel_size=[3,3,3],padding='SAME')\n",
    "            x = tf.layers.dropout(x,rate=0.1,training=training)\n",
    "            x = tf.layers.batch_normalization(x,training=training,renorm=True)\n",
    "            x = tf.nn.relu(x)\n",
    "            x_con = tf.concat([x,x_con], axis = 4)\n",
    "        if pool is False:\n",
    "            return x_con\n",
    "        \n",
    "        #x = tf.layers.conv3d(x_con,12,kernel_size=[1,1,1],padding='SAME')\n",
    "        #x = tf.layers.dropout(x,rate=0.1,training=training)\n",
    "        #x = tf.layers.batch_normalization(x,training=training,renorm=True)\n",
    "        #x = tf.nn.relu(x)\n",
    "        pool = tf.layers.max_pooling3d(x_con,pool_size = [2,2,1], strides=[2,2,1],data_format='channels_last')\n",
    "\n",
    "        return x_con, pool\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "#Dense-Unet model:  -- uses a dice loss function along w/ softmax cross entropy;  \n",
    "class DUnet():\n",
    "    def __init__(self, x, training):\n",
    "        #self.filters = filters\n",
    "        self.training = training\n",
    "        self.model = self.DU_net(x)\n",
    "\n",
    "    \n",
    "    def DU_net(self,input_):\n",
    "\n",
    "\n",
    "        conv1, pool1 = encoder_layer(input_,iterations=2,name=\"encode_\"+str(1),training=self.training, pool=True)\n",
    "        conv2, pool2 = encoder_layer(pool1,iterations=4,name=\"encode_\"+str(2),training=self.training, pool=True)\n",
    "        conv3, pool3 = encoder_layer(pool2,iterations=6,name=\"encode_\"+str(3),training=self.training, pool=True)\n",
    "        conv4, pool4 = encoder_layer(pool3,iterations=8,name=\"encode_\"+str(4),training=self.training, pool=True)\n",
    "        conv5, pool5 = encoder_layer(pool4,iterations=10,name=\"encode_\"+str(5),training=self.training, pool=True)\n",
    "        conv6 = encoder_layer(pool5,iterations=12,name=\"encode_\"+str(5),training=self.training, pool=False)\n",
    "        up1 = decoder_layer(conv6,conv5,10,name=1)\n",
    "        conv7 = encoder_layer(up1,iterations=10,name=\"conv\"+str(6),training=self.training, pool=False)\n",
    "        up2 = decoder_layer(conv7,conv4,8,name=2)\n",
    "        conv8 = encoder_layer(up2,iterations=8,name=\"encode_\"+str(7),training=self.training, pool=False)\n",
    "        up3 = decoder_layer(conv8,conv3,6,name=3)\n",
    "        conv9 = encoder_layer(up3,iterations=6,name=\"encode_\"+str(8),training=self.training, pool=False)\n",
    "        up4 = decoder_layer(conv9,conv2,4,name=4)\n",
    "        conv10 = encoder_layer(up4,iterations= 4,name=\"encode_\"+str(9),training=self.training, pool=False)\n",
    "        up5 = decoder_layer(conv10,conv1,2,name=5)\n",
    "        conv11 = encoder_layer(up5,iterations= 2,name=\"encode_\"+str(10),training=self.training, pool=False)\n",
    "\n",
    "        #The final layer to generate a probability map from the CNN: channels input should be total number of classes (ie. a binary segmentation of 0 or 1 has a total number of classes = 2)\n",
    "        score = tf.layers.conv3d(conv11,2,(1,1,1),name='logits',padding='SAME')\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "To test dataset on the network and weights from dense_unet.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "###################################\n",
    "#Function to decipher tfrecord of test dataset\n",
    "def feed_data(pathData):\n",
    "    data_path = pathData  # path to testing tfrecord\n",
    "    feature = {'test/image': tf.FixedLenFeature([], tf.string),\n",
    "               'test/label': tf.FixedLenFeature([], tf.string),\n",
    "               'test/depth': tf.FixedLenFeature([], tf.int64),\n",
    "               'test/height': tf.FixedLenFeature([], tf.int64),\n",
    "               'test/width': tf.FixedLenFeature([], tf.int64),\n",
    "               'test/velocity': tf.FixedLenFeature([], tf.string) }\n",
    "    \n",
    "    # Create a list of filenames and pass it to a queue\n",
    "    filename_queue = tf.train.string_input_producer([data_path])\n",
    "\n",
    "    # Define a reader and read the next record\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "    # Decode the record read by the reader\n",
    "    features = tf.parse_single_example(serialized_example, features=feature)\n",
    "    height = tf.cast(features[\"test/height\"], tf.int32)\n",
    "    width = tf.cast(features[\"test/width\"], tf.int32)\n",
    "    depth = tf.cast(features[\"test/depth\"], tf.int32)\n",
    "\n",
    "    # Convert the image data from string back to the numbers\n",
    "    image = tf.decode_raw(features['test/image'], tf.float32)\n",
    "    \n",
    "    # Cast label data from string back to the numbers\n",
    "    label = tf.decode_raw(features['test/label'], tf.float32)\n",
    "\n",
    "    # Reshape image data into the original shape\n",
    "    image = tf.reshape(image, [height, width,depth])\n",
    "    label = tf.reshape(label, [height, width,depth])\n",
    "\n",
    "    #Crop image for input size if desired\n",
    "    image = tf.image.resize_image_with_crop_or_pad(image, size1,size2)\n",
    "    label = tf.image.resize_image_with_crop_or_pad(label, size1, size2)\n",
    "    #image = tf.image.resize_image_with_crop_or_pad(image, 128, 96)\n",
    "    #label = tf.image.resize_image_with_crop_or_pad(label, 128, 96)\n",
    "    #image = image[172:268]\n",
    "    #label = label[172:268]\n",
    "    #Cast image and label to float32\n",
    "    label = tf.cast(label,tf.float32)\n",
    "    image = tf.cast(image,tf.float32)\n",
    "\n",
    "    #This hard-codes batch-size to 1 for testing--would need to rewrite input pipeline to enable batch_size > 1\n",
    "    image = tf.expand_dims(image,axis = 0)\n",
    "    label = tf.expand_dims(label,axis = 0)\n",
    "    image = tf.expand_dims(image,axis = 4) #Add grayscale channel to input image (ie. image_shape = HxWx1 for gray or HxWx3 for color)\n",
    "\n",
    "    image = (image - tf.reduce_min(image))/(tf.reduce_max(image) - tf.reduce_min(image)) #Normalize input image\n",
    "\n",
    "    q = tf.FIFOQueue(capacity=50, dtypes=[tf.float32, tf.float32])# set up queue\n",
    "    enqueue_op = q.enqueue_many([image,label])\n",
    "    qr = tf.train.QueueRunner(q,[enqueue_op])\n",
    "\n",
    "    return image, label, height, width "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################\n",
    "#Calculate the Dice score between label and CNN output\n",
    "def cost_dice(logits, labels,name='cost'):\n",
    "    with tf.name_scope('cost'):\n",
    "        eps = 1e-5\n",
    "        #N,H,W,C,J = labels.get_shape()\n",
    "        logits = tf.nn.softmax(logits)\n",
    "        logits = logits[...,1]>=0.5 #used a threshold value of 0.5 for dice calculations\n",
    "        #logits = logits[...,1]\n",
    "        logits = tf.cast(logits,tf.float32)\n",
    "        labels = labels\n",
    "\n",
    "        log = tf.reshape(logits,[1,-1])\n",
    "        \n",
    "        labels = tf.reshape(labels,[1,-1])\n",
    "        \n",
    "        inte = tf.multiply(log,labels)\n",
    "        inter = eps + tf.reduce_sum(inte)\n",
    "        union =  tf.reduce_sum(log) + tf.reduce_sum(labels)+eps\n",
    "        dice = tf.reduce_mean(2* inter/ (union))\n",
    "        #loss = 1- loss\n",
    "        return dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Unet_test(pathData,num, path, pathSave):\n",
    "\n",
    "    #image_batch_placeholder = tf.placeholder(tf.float32, shape=[None, 128, 96,None, 1])\n",
    "    #label_batch_placeholder = tf.placeholder(tf.float32, shape=[None, 128, 96,None])\n",
    "    image_batch_placeholder = tf.placeholder(tf.float32, shape=[None, size1, size2,None, 1])\n",
    "    label_batch_placeholder = tf.placeholder(tf.float32, shape=[None, size1, size2,None])\n",
    "    \n",
    "    labels_pixels = tf.reshape(label_batch_placeholder, [-1, 1])   \n",
    "    training_flag = tf.placeholder(tf.bool)\n",
    "    image_batch,label_batch,height, width = feed_data(pathData)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    logits = DUnet(x = image_batch_placeholder, training=training_flag).model #Runs CNN, logits = raw CNN output\n",
    "    #logits = logits>1\n",
    "    logs = cost_dice(logits,label_batch_placeholder)\n",
    "\n",
    "    llogits = tf.nn.softmax(logits) #Apply softmax to CNN output to generate probability map\n",
    "    \n",
    "    \n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(path) #Path to the pre-trained weights that you want to test\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    config = tf.ConfigProto(log_device_placement=False)\n",
    "    all_trainable_vars = tf.reduce_sum([tf.reduce_prod(v.shape) for v in tf.trainable_variables()])\n",
    "\n",
    "    config.gpu_options.allow_growth=True\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tf.logging.info(\"Restoring full model from checkpoint file %s\",checkpoint.model_checkpoint_path)\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord, sess = sess)\n",
    "        ave = [] #To calculate average dice scores\n",
    "        ma = []  #Collect a vector of all test data CNN outputs for ROC graph and AUC calculation\n",
    "        tru = [] #Collect a vector of all test data labels for ROC graph and AUC calculation\n",
    "        use = [] #Save a matstruct of all masks from CNN\n",
    "        gtruth = [] #Save a matstruct of all labels\n",
    "        low = [] #Save a matstruct of all masks from CNN w/ a dice score < 0.9\n",
    "        low_true = [] #Save a matstruct of all labels w/ a dice score < 0.9\n",
    "        low_loss = [] #To calculate average dice scores of datasets w/ a dice score < 0.9\n",
    "        total = 15 #Number of testing datasets\n",
    "        for i in tqdm(range(int(total))):\n",
    "            image_out, truth = sess.run([image_batch, label_batch])\n",
    "            \n",
    "            _, dice, llogit = sess.run([logits, logs, llogits], feed_dict={image_batch_placeholder: image_out,\n",
    "                                                                                    label_batch_placeholder: truth,\n",
    "                                                                                    training_flag: True})\n",
    "            \n",
    "\n",
    "           \n",
    "            infer_out = llogit[...,1] > 0.5 #CNN mask w/ a threshold of 0.5 (ie. (pixels > 0.5) = 1, else pixels = 0)\n",
    "            \n",
    "\n",
    "            O = np.float32(image_out) #Orginal image\n",
    "          \n",
    "            Y = np.float32(truth) #Orginal label/mask\n",
    "            \n",
    "            data = np.squeeze(infer_out)\n",
    "            truth = np.squeeze(truth)\n",
    "            dice = dice\n",
    "            if dice <1.00:\n",
    "                print(dice)\n",
    "                print(i)\n",
    "                low_d = data.transpose()\n",
    "                low.append(low_d)\n",
    "                low_t = truth.transpose()\n",
    "                low_true.append(low_t)\n",
    "                low_loss.append(dice)\n",
    "            ave.append(dice)\n",
    "            #print(ys.shape)\n",
    "           \n",
    "            #gt = np.squeeze(Y)\n",
    "\n",
    "\n",
    "            mask = data.transpose()\n",
    "            gtuse = truth.transpose()\n",
    "            gtruth.append(gtuse)\n",
    "            use.append(mask)\n",
    "            #gt = gt.transpose()\n",
    "            da = np.reshape(data,(-1,1))\n",
    "            gt = np.reshape(truth,(-1,1))\n",
    "            ma.append(da)\n",
    "            tru.append(gt)\n",
    "           \n",
    "            \n",
    "            G = (infer_out)\n",
    "            #G = np.reshape(G,(256,256))\n",
    "            U = np.squeeze(Y)\n",
    "            W = np.squeeze(G)\n",
    "            lo = np.squeeze(O)\n",
    "            #ys = (ys)\n",
    "            #print(W.shape)\n",
    "            #plt.imshow(U)\n",
    "            #plt.show()\n",
    "            #plt.pause(0.1)\n",
    "            #plt.imshow(W)\n",
    "            #plt.show()\n",
    "            #plt.pause(0.1)\n",
    "            \n",
    "            #plt.imshow(lo)\n",
    "            #plt.show()\n",
    "\n",
    "            #if(i == 0):\n",
    "            #segmentor = \"Amanda\"\n",
    "            #else:\n",
    "            #    segmentor = \"Liliana\"\n",
    "\n",
    "            #strPath = pathSave + '/mask' + segmentor + num + '.mat'\n",
    "            #io.savemat(strPath,{'data':use})\n",
    "            #strPath = pathSave + '/low' + segmentor + num + '.mat'\n",
    "            #io.savemat(strPath,{'data':low})\n",
    "            #strPath = pathSave + '/low_truth' + segmentor + num + '.mat'\n",
    "            #io.savemat(strPath,{'truth':low_true})\n",
    "            #strPath = pathSave + '/input_image' + segmentor + num + '.mat'\n",
    "            #io.savemat(strPath,{'input':image_out})\n",
    "            #strPath = pathSave+ '/gt' + segmentor + num + '.mat'\n",
    "            #io.savemat(strPath,{'truth':gtruth})\n",
    "        \n",
    "        strPath = pathSave + '/mask' + num + '.mat'\n",
    "        io.savemat(strPath,{'data':use})\n",
    "        strPath = pathSave + '/low' + num + '.mat'\n",
    "        io.savemat(strPath,{'data':low})\n",
    "        strPath = pathSave + '/low_truth' + num + '.mat'\n",
    "        io.savemat(strPath,{'truth':low_true})\n",
    "        strPath = pathSave + '/input_image' + num + '.mat'\n",
    "        io.savemat(strPath,{'input':image_out})\n",
    "        strPath = pathSave+ '/gt'  + num + '.mat'\n",
    "        io.savemat(strPath,{'truth':gtruth})\n",
    "\n",
    "        \n",
    "        \n",
    "        print(sess.run(all_trainable_vars))\n",
    "\n",
    "        print(sum(ave)/len(ave)) #Average dice scores for testing dataset\n",
    "\n",
    "        \n",
    "       \n",
    "        \n",
    "\n",
    "        #print(statistics.stdev(ave))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        tf.train.write_graph(sess.graph_def, 'graph/', 'my_graph.pb', as_text=False)\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    for cv in range(0,5):\n",
    "        num = str(cv+1)\n",
    "        path = \"./MagVelLoss/trainingModelCV\" + str(num)\n",
    "        print(path)\n",
    "        \n",
    "        pathData = r\"D:\\LA_Vel_Loss\\data_formatting\\test_3dCV\" + str(cv+1) + \".tfrecords\"\n",
    "        print(pathData)\n",
    "        savePath = r\"D:\\LA_Vel_Loss\\data_results_mag_vel_loss\"\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        Unet_test(pathData, num, path, savePath)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./MagVelLoss/trainingModelCV1\n",
      "D:\\LA_Vel_Loss\\data_formatting\\test_3dCV1.tfrecords\n",
      "INFO:tensorflow:Restoring full model from checkpoint file ./MagVelLoss/trainingModelCV1\\hb.ckpt-15001\n",
      "INFO:tensorflow:Restoring parameters from ./MagVelLoss/trainingModelCV1\\hb.ckpt-15001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:03<00:46,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010237512\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:03<00:32,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1293233\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:04<00:24,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00062656956\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:05<00:17,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.591368e-09\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [00:06<00:13,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17050946\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:06<00:10,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9351335e-09\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:07<00:08,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.054328e-09\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [00:08<00:07,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26787513\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [00:09<00:06,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023146598\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [00:10<00:04,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030755317\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [00:11<00:03,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09013509\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [00:12<00:02,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22485268\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [00:12<00:01,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09456135\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [00:13<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8175223e-09\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:14<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0029473705\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6118924\n",
      "0.0696646935288704\n",
      "./MagVelLoss/trainingModelCV2\n",
      "D:\\LA_Vel_Loss\\data_formatting\\test_3dCV2.tfrecords\n",
      "INFO:tensorflow:Restoring full model from checkpoint file ./MagVelLoss/trainingModelCV2\\hb.ckpt-15001\n",
      "INFO:tensorflow:Restoring parameters from ./MagVelLoss/trainingModelCV2\\hb.ckpt-15001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:03<00:42,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1318508e-09\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:03<00:31,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19609644\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:04<00:22,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1296712\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:05<00:17,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10536444\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [00:06<00:12,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008413539\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:06<00:10,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1063717e-09\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:07<00:08,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.034421366\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [00:08<00:06,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06913128\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [00:09<00:05,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07066859\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [00:10<00:04,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.695652e-10\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [00:10<00:03,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1585596e-09\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [00:11<00:02,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06580305\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [00:12<00:01,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1633503\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [00:13<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.084446646\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:13<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.112356e-09\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6118924\n",
      "0.06182445748967003\n",
      "./MagVelLoss/trainingModelCV3\n",
      "D:\\LA_Vel_Loss\\data_formatting\\test_3dCV3.tfrecords\n",
      "INFO:tensorflow:Restoring full model from checkpoint file ./MagVelLoss/trainingModelCV3\\hb.ckpt-15001\n",
      "INFO:tensorflow:Restoring parameters from ./MagVelLoss/trainingModelCV3\\hb.ckpt-15001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:03<00:42,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6357242e-09\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:03<00:30,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.047940426\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:04<00:22,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15774265\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:05<00:16,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.051782236\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [00:05<00:12,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.241619e-09\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:06<00:09,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17324351\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:07<00:08,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11614654\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [00:07<00:06,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0023629935\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [00:08<00:05,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2615233\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [00:09<00:04,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0039080833\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [00:10<00:03,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.046652615\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [00:11<00:02,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6455487e-09\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [00:11<00:01,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08418751\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [00:12<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012231482\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:13<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2968677\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6118924\n",
      "0.08363927008934431\n",
      "./MagVelLoss/trainingModelCV4\n",
      "D:\\LA_Vel_Loss\\data_formatting\\test_3dCV4.tfrecords\n",
      "INFO:tensorflow:Restoring full model from checkpoint file ./MagVelLoss/trainingModelCV4\\hb.ckpt-15001\n",
      "INFO:tensorflow:Restoring parameters from ./MagVelLoss/trainingModelCV4\\hb.ckpt-15001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:02<00:41,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.061305977\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:03<00:29,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35463226\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:04<00:22,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.469124\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:05<00:16,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32781297\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [00:06<00:13,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00024981514\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:07<00:13,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13127413\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:08<00:10,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.363683\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [00:10<00:09,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5182574e-09\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [00:11<00:07,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0054145e-09\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [00:13<00:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19321865\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [00:13<00:04,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2883419\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [00:14<00:03,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01162952\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [00:15<00:02,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0861296\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [00:16<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030503685\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:16<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07513417\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6118924\n",
      "0.1595359778544559\n",
      "./MagVelLoss/trainingModelCV5\n",
      "D:\\LA_Vel_Loss\\data_formatting\\test_3dCV5.tfrecords\n",
      "INFO:tensorflow:Restoring full model from checkpoint file ./MagVelLoss/trainingModelCV5\\hb.ckpt-15001\n",
      "INFO:tensorflow:Restoring parameters from ./MagVelLoss/trainingModelCV5\\hb.ckpt-15001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:03<00:43,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1220116e-09\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:04<00:31,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0026965106\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:04<00:22,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4157458\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [00:05<00:18,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028921027\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [00:06<00:14,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11837612\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:07<00:12,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10686665\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [00:08<00:09,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.072324e-09\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [00:09<00:07,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22829449\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [00:10<00:06,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15746476\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [00:10<00:04,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24585417\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [00:11<00:03,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11442112\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [00:12<00:02,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011993058\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [00:12<00:01,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.076735616\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [00:13<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7432155e-09\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:14<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37824827\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6118924\n",
      "0.12570784010740396\n",
      "All done :)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    print(\"All done :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
